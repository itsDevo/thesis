# Goal of the Thesis
The goal of this thesis is to explore and compare the performance of Q-learning, a reinforcement learning algorithm, with traditional pathfinding algorithms (Dijkstra’s Algorithm and A*) in solving maze navigation problems. The mazes will increase in complexity across three stages (4x4, 6x6, and 8x8), featuring walls and dead ends to challenge the algorithms. The comparison will be based on three key metrics: time to solve, number of steps, and success rate. Additionally, Q-learning-specific metrics such as reward convergence and exploration vs. exploitation will be analyzed to understand the learning process. The project will also include a random maze generator to ensure robust testing and reproducibility. The results will be visualized to provide insights into the strengths and weaknesses of each algorithm.

# Structure of the Thesis
## Introduction:

Overview of reinforcement learning and Q-learning.

Introduction to traditional pathfinding algorithms (Dijkstra, A*).

## Problem statement: Maze navigation with increasing complexity.

Objectives of the thesis.

Background and Literature Review:

Explanation of Q-learning, Dijkstra’s Algorithm, and A*.

Discussion of related work in maze-solving and algorithm comparisons.

## Methodology:

Description of the maze environment and stages.

Implementation details for Q-learning, Dijkstra, and A*.

Explanation of the random maze generator.

Metrics used for evaluation (time to solve, steps, success rate, reward convergence, etc.).

## Implementation:

Overview of the code structure and files.

Details on how each algorithm was implemented and tested.

Explanation of visualization methods.

## Results and Analysis:

Comparison of algorithm performance across stages.

Analysis of Q-learning metrics (reward convergence, exploration vs. exploitation).

Visualization of results (e.g., success rate over episodes, steps to solve).

## Discussion:

Interpretation of results.

Strengths and weaknesses of each algorithm.

Limitations of the study and potential improvements.

## Conclusion:

Summary of findings.

Implications for future work in reinforcement learning and pathfinding.

## References:

Citations for all literature and resources used.
